<html>
    <head>
        <h1>This document outlines the general strategy behind building E.S.R.A. (acronym standing for E.xtremely S.ociable R.obotic A.gent)</h1>
    </head>
    <body>
        <p>The three main building blocks of ESRA are 
            <ul>
                <li>Physics/Materials --- human analogy: Body</li>
                <li>Control System --- human analogy: Nervous System</li>
                <li>Data --- human analogy: Brain</li>
            </ul>
        </p>
        <br>
        <p>***********************************************************************************************</p>
        <br>
        <p>These three main building blocks will be driven by following materials / methods:</p>
            <ul>
                <li>BODY:</li>
                <ul>
                    <li>TENDONS INSTEAD OF RIGID JOINTS</li>
                    <li>SOFT ROBOTIC MATERIALS, SUCH AS SILICONE ("ARTIFICIAL SKIN")</li>
                    <li>DEFORMABLE MATERIALS, SUCH AS POLYMORPH ("ARTIFICIAL BONES")</li>
                </ul>
                <br>
                <li>NERVOUS SYSTEM:</li>
                <ul>
                    <li>PAM ACTUATORS (SOFT ACTUATORS IN GENERAL)</li>
                    <li>MICROCONTROLLER (ARDUINO)</li>
                    <li>TINY ML APPLICATIONS ON MICROCONTROLLER</li>
                </ul>
                <br>
                <li>BRAIN:</li>
                <ul>
                    <li>TINY ML APPLICATIONS FOR INSTANT PROCESSING</li>
                    <li>MORE SOPHISTICATED CLOUD-BASED AI</li>
                </ul>
            </ul>
            <br>
            <div>
                <h1>Food for Thought:</h1>
                <ul>
                    <li>Allow imperfection and imprecision instead of enforcing precision and "correctness" all the time,
                        uncertainty is something positive.
                    </li>
                    <li>The core working principle is modularity: Concentrate on the whole system all the time (holistic),
                        but solve each piece of the puzzle one at a time.
                    </li>
                    <li>Always evaluate the easiest feasible implementation first, it might work (A version of Occam's Razor)</li>
                    <li>Minutely document each step</li>
                    <li>Don't reinvent the wheel when there is a (non-proprietary) solution available</li>
                    <li>Learn to walk with an RL approach? Solve problems of balance and autonomy.</li>
                    <li>Make use of affordable control mechanisms for PAMs</li>
                </ul>
            </div>
            <br><br>
            <div>
                Consider the head as the first project part:
                Think of three 'layers' of design -->
                <br>
                <ul>
                    <li><b>First layer: Appearance/'Feel'</b>
                        <br>
                        Create an initially static head that can potentially pass a 'Physical Turing Test'
                        <ul>1. Sculpting/modeling face and head</ul>
                        <ul>2. Use 'realistic' elastomer to create a human-like mask</ul>
                        <ul>3. Put skull and face (mask) together to make correct actuation possible</ul>
                    </li>
                    <br>
                    <li><b>Second layer: Mimicry/'Aliveness'</b>
                        <br>
                        Bring the head to life by imitating an array of human facial expressions (check Paul Ekman)
                        <ul>1. Facial expressions</ul>
                        <ul>2. Eye expressions</ul>
                        <ul>3. Lip sync while talking -- could be a very challenging task (jaw mechanism)</ul>
                        <ul>4. Head movement using a neck mechanism</ul>
                        The challenge is to find the most appropriate actuation: Servos, PAM, other soft actuators, tendons ???
                        Another crucial task is to actuate the mask autonomously (untethered) in accordance with head movement.
                    </li>
                    <br>
                    <li><b>Third layer: Brain/'Intelligence'</b>
                        <br>
                        Deploy a mixture of 'TinyML', more sophisticated CPU-based or cloud-based AI and sensors to 
                        <ul>1. Make the head react to human contact (counter-react a human expression with an appropriate counter-expression)</ul>
                        <ul>2. 'Wake up' (see Alexa) -- TinyML</ul>
                        <ul>3. Recognize known interlocutors -- TinyML</ul>
                        <ul>4. See and identify things -- traditional ML</ul>
                        <ul>5. Talk / engage in a purposeful conversation -- here, deterministic methods such as AIML must be considered vs. ML-based NLP.
                            Furthermore, evaluate text-to-speech (TTS) and, reversely, speech recognition (STT) methods.  
                            Possible concrete implementation of STT --> Chatbot --> TTS:
                            <li>
                                <ul>1. Use Mozilla DeepSpeech to capture microphone audio and infer the content. Then save it to a string</ul>
                                <ul>2. Use that string as input ('question') for an AIML-based chatbot</ul>
                                <ul>3. Process the reply in pyttsx3 for subsequent TTS voice output</ul>
                            </li>                      
                        </ul>
                        <br>
                        With certain exceptions, all three described layers can be developed more or less independently from one another.
                    </li>
                </ul>
            </div>
    </body>
</html>